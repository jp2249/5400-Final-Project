{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texas Death Row Data Processing\n",
    "\n",
    "This notebook combines the functionality of scraping Texas executed offenders' last statements and cleaning the collected data.\n",
    "\n",
    "## Parts\n",
    "1. **Data Collection**: Scrapes data from the Texas Department of Criminal Justice website.\n",
    "2. **Data Cleaning**: Processes the text of the last statements to remove unwanted context and flag religious content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAIN = \"https://www.tdcj.texas.gov\"\n",
    "MAIN_URL = \"https://www.tdcj.texas.gov/death_row/dr_executed_offenders.html\"\n",
    "\n",
    "def get_last_statement(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 404:\n",
    "            return \"Page not found\"\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        content_body = soup.find(id='content_right')\n",
    "        if not content_body:\n",
    "             content_body = soup.find(id='body')\n",
    "\n",
    "        if content_body:\n",
    "            paragraphs = content_body.find_all('p')\n",
    "            statement_text = []\n",
    "            recording = False\n",
    "            \n",
    "            for p in paragraphs:\n",
    "                text = p.get_text(strip=True)\n",
    "                if \"Last Statement:\" in text:\n",
    "                    parts = text.split(\"Last Statement:\", 1)\n",
    "                    if len(parts) > 1 and parts[1].strip():\n",
    "                        statement_text.append(parts[1].strip())\n",
    "                    recording = True\n",
    "                elif recording:\n",
    "                    # Stop if we hit other sections usually at the bottom\n",
    "                    if \"Date of Execution:\" in text or \"Offender:\" in text:\n",
    "                        continue\n",
    "                    statement_text.append(text)\n",
    "            \n",
    "            return \" \".join(statement_text)\n",
    "\n",
    "        return \"Content body not found\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching statement: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching main page: https://www.tdcj.texas.gov/death_row/dr_executed_offenders.html\n",
      "Found 597 rows. Processing...\n",
      "Processed 0 rows...\n",
      "Processed 10 rows...\n",
      "Processed 20 rows...\n",
      "Processed 30 rows...\n",
      "Processed 40 rows...\n",
      "Processed 50 rows...\n",
      "Processed 60 rows...\n",
      "Processed 70 rows...\n",
      "Processed 80 rows...\n",
      "Processed 90 rows...\n",
      "Processed 100 rows...\n",
      "Processed 110 rows...\n",
      "Processed 120 rows...\n",
      "Processed 130 rows...\n",
      "Processed 140 rows...\n",
      "Processed 150 rows...\n",
      "Processed 160 rows...\n",
      "Processed 170 rows...\n",
      "Processed 180 rows...\n",
      "Processed 190 rows...\n",
      "Processed 200 rows...\n",
      "Processed 210 rows...\n",
      "Processed 220 rows...\n",
      "Processed 230 rows...\n",
      "Processed 240 rows...\n",
      "Processed 250 rows...\n",
      "Processed 260 rows...\n",
      "Processed 270 rows...\n",
      "Processed 280 rows...\n",
      "Processed 290 rows...\n",
      "Processed 300 rows...\n",
      "Processed 310 rows...\n",
      "Processed 320 rows...\n",
      "Processed 330 rows...\n",
      "Processed 340 rows...\n",
      "Processed 350 rows...\n",
      "Processed 360 rows...\n",
      "Processed 370 rows...\n",
      "Processed 380 rows...\n",
      "Processed 390 rows...\n",
      "Processed 400 rows...\n",
      "Processed 410 rows...\n",
      "Processed 420 rows...\n",
      "Processed 430 rows...\n",
      "Processed 440 rows...\n",
      "Processed 450 rows...\n",
      "Processed 460 rows...\n",
      "Processed 470 rows...\n",
      "Processed 480 rows...\n",
      "Processed 490 rows...\n",
      "Processed 500 rows...\n",
      "Processed 510 rows...\n",
      "Processed 520 rows...\n",
      "Processed 530 rows...\n",
      "Processed 540 rows...\n",
      "Processed 550 rows...\n",
      "Processed 560 rows...\n",
      "Processed 570 rows...\n",
      "Processed 580 rows...\n",
      "Processed 590 rows...\n",
      "Collected 596 records.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Fetching main page: {MAIN_URL}\")\n",
    "try:\n",
    "    response = requests.get(MAIN_URL)\n",
    "    response.raise_for_status()\n",
    "except Exception as e:\n",
    "    print(f\"Failed to fetch main page: {e}\")\n",
    "    # Stop execution if main page fails, practically raising error here for notebook flow\n",
    "    raise e\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "table = soup.find('table')\n",
    "if not table:\n",
    "    print(\"Could not find the table.\")\n",
    "else:\n",
    "    rows = table.find_all('tr')\n",
    "    data_to_save = []\n",
    "    print(f\"Found {len(rows)} rows. Processing...\")\n",
    "\n",
    "    for i, row in enumerate(rows[1:]): # Skip header\n",
    "        cols = row.find_all('td')\n",
    "        if len(cols) < 6:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            last_name = cols[3].get_text(strip=True)\n",
    "            first_name = cols[4].get_text(strip=True)\n",
    "            tdcj_number = cols[5].get_text(strip=True)\n",
    "            \n",
    "            last_statement_link = None\n",
    "            link_col = cols[2]\n",
    "            a_tag = link_col.find('a')\n",
    "            if a_tag:\n",
    "                href = a_tag.get('href')\n",
    "                if href:\n",
    "                    if href.startswith('/'):\n",
    "                        last_statement_link = f\"{DOMAIN}{href}\"\n",
    "                    elif href.startswith('http'):\n",
    "                        last_statement_link = href\n",
    "                    else:\n",
    "                        # Relative to /death_row/\n",
    "                        last_statement_link = f\"{DOMAIN}/death_row/{href}\"\n",
    "            \n",
    "            statement = \"N/A\"\n",
    "            if last_statement_link:\n",
    "                statement = get_last_statement(last_statement_link)\n",
    "            \n",
    "            data_to_save.append({\n",
    "                \"TDCJ Number\": tdcj_number,\n",
    "                \"First Name\": first_name,\n",
    "                \"Last Name\": last_name,\n",
    "                \"Last Statement URL\": last_statement_link,\n",
    "                \"Last Statement\": statement,\n",
    "                \"is_criminal\": 1,\n",
    "                \"is_expected\": 1,\n",
    "                \"is_religious\": 0\n",
    "            })\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print(f\"Processed {i} rows...\")\n",
    "                # Optional: break for testing\n",
    "                # if i > 20: break \n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing row {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Convert to DataFrame in memory instead of saving intermediate CSV\n",
    "    df = pd.DataFrame(data_to_save)\n",
    "    print(f\"Collected {len(df)} records.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_statement(text):\n",
    "    \"\"\"\n",
    "    Cleans the 'Last Statement' column by setting specific 'no statement' phrases to None.\n",
    "    Also handles NaN/float values.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    \n",
    "    # Normalize text for checking\n",
    "    text_str = str(text).strip()\n",
    "    \n",
    "    # Exact phrases or startswith patterns to identify no statement\n",
    "    no_statement_patterns = [\n",
    "        \"No last statement given\",\n",
    "        \"No\",\n",
    "        \"No, last statement given\",\n",
    "        \"Content body not found\",\n",
    "        \"No last statement.\",\n",
    "        \"No last statement given.\",\n",
    "        \"No statement given.\",\n",
    "        \"Page not found\",\n",
    "        \"None\",\n",
    "        \"None.\",\n",
    "        \"N/A\",\n",
    "        \"This inmate declined to make a last statement\", \n",
    "        \"this inmate inmate decline to make a last stattment\", \n",
    "    ]\n",
    "    \n",
    "    for pattern in no_statement_patterns:\n",
    "        # Case insensitive check\n",
    "        if text_str.lower().startswith(pattern.lower()):\n",
    "            return None\n",
    "            \n",
    "    return text_str\n",
    "\n",
    "def remove_context_phrases(text):\n",
    "    \"\"\"\n",
    "    Removes specific context phrases from the start or body of the statement,\n",
    "    maintaining the rest of the sentence.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text is None:\n",
    "        return text\n",
    "        \n",
    "    # Phrases to remove\n",
    "    phrases = [\n",
    "        r\"\\(written statement\\)\",\n",
    "        r\"Spoken:\",\n",
    "        r\"written:\",\n",
    "        r\"Verbal statement:\",\n",
    "        r\"\\(Spanish\\)\",\n",
    "        r\"Statement to the Media:\",\n",
    "        r\"High Flight \\(aviation poem\\)\",\n",
    "        r\"\\(First two or three words not understood\\.\\)\",\n",
    "        r\"1 Corinthians 12:31B – 13:13 \\(NIV\\)\",\n",
    "        r\"1 Corinthians 12:31B – 13:13 \\(NIV\\) \",\n",
    "        r\"I would just...\\(speaking in French\\)\",\n",
    "        r\"He spoke in Irish, translating to\",\n",
    "        r\"English:\",\n",
    "        r\"\\(Mumbled\\.\\)\"\n",
    "    ]\n",
    "    \n",
    "    # Join into a single pattern\n",
    "    pattern = \"|\".join(phrases)\n",
    "    \n",
    "    # Replace with empty string, preserving the rest\n",
    "    cleaned_text = re.sub(pattern, \"\", str(text), flags=re.IGNORECASE)\n",
    "    \n",
    "    return cleaned_text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_religious(text):\n",
    "    \"\"\"\n",
    "    Detects if the last statement contains religious keywords.\n",
    "    Returns True if found, False otherwise.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return 0\n",
    "        \n",
    "    keywords = [\n",
    "        'god', 'lord', 'jesus', 'christ', 'allah', 'holy', 'pray', 'prayer', \n",
    "        'heaven', 'bible', 'scripture', 'amen', 'bless', 'faith', 'salvation', \n",
    "        'redemption', 'psalm', 'islam', 'muslim', 'christian'\n",
    "    ]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        if keyword in text_lower:\n",
    "            return 1\n",
    "            \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning 'Last Statement' column...\n",
      "Removing context phrases...\n",
      "Creating 'is_religious' column...\n",
      "Saving cleaned data to: ../data/raw_data/texas_last_statements_labeled.csv\n",
      "------------------------------\n",
      "Processing Complete.\n",
      "Total records: 596\n",
      "Records with No Statement (None): 125\n",
      "Records marked as Religious: 260\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "output_path = os.path.join(\"../data/raw_data\", \"texas_last_statements_labeled.csv\")\n",
    "\n",
    "# Check if df exists (from Part 1)\n",
    "if 'df' not in locals():\n",
    "    print(\"Error: DataFrame 'df' not found. Run Part 1 first.\")\n",
    "else:\n",
    "    print(\"Cleaning 'Last Statement' column...\")\n",
    "\n",
    "    # Apply removal of context phrases FIRST\n",
    "    print(\"Removing context phrases...\")\n",
    "    df['Last Statement'] = df['Last Statement'].apply(remove_context_phrases)\n",
    "\n",
    "    # Apply cleaning (no statement checks)\n",
    "    df['Last Statement'] = df['Last Statement'].apply(clean_statement)\n",
    "\n",
    "    print(\"Creating 'is_religious' column...\")\n",
    "    # Apply religious detection\n",
    "    df['is_religious'] = df['Last Statement'].apply(detect_religious)\n",
    "\n",
    "    print(f\"Saving cleaned data to: {output_path}\")\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "    # Verification stats\n",
    "    total = len(df)\n",
    "    religious_count = df['is_religious'].sum()\n",
    "    none_count = df['Last Statement'].isna().sum()\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Processing Complete.\")\n",
    "    print(f\"Total records: {total}\")\n",
    "    print(f\"Records with No Statement (None): {none_count}\")\n",
    "    print(f\"Records marked as Religious: {religious_count}\")\n",
    "    print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
