{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f626ea20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3098, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3153, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3362, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3607, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/sz/m07m37v53zbbxzbhqb8tjxjh0000gn/T/ipykernel_41361/2979242597.py\", line 2, in <module>\n",
      "    from transformers import pipeline\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/transformers/__init__.py\", line 27, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/transformers/utils/__init__.py\", line 24, in <module>\n",
      "    from .auto_docstring import (\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/transformers/utils/auto_docstring.py\", line 30, in <module>\n",
      "    from .generic import ModelOutput\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/transformers/utils/generic.py\", line 51, in <module>\n",
      "    import torch  # noqa: F401\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/tyler/anaconda3/envs/dsan5400/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import argparse\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e97cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionAnalyzer:\n",
    "\n",
    "    def __init__(self, df, text_col, model_name =\"SamLowe/roberta-base-go_emotions\", word_chunk_size =250):\n",
    "        self.df = df\n",
    "        self.text_col = text_col\n",
    "        self.model_name = model_name\n",
    "        self.word_chunk_size = word_chunk_size\n",
    "        self.pipeline = pipeline(\"text-classification\", model=self.model_name)\n",
    "        self.pipeline_top_k = pipeline(\"text-classification\", model=self.model_name, top_k=None)\n",
    "\n",
    "    def chunk_text(self, text):\n",
    "        \"\"\"Chunk text into smaller pieces in case of long texts\"\"\"\n",
    "        words = str(text).split()\n",
    "        return [\n",
    "            \" \".join(words[i:i + self.word_chunk_size])\n",
    "            for i in range(0, len(words), self.word_chunk_size)\n",
    "        ]\n",
    "    \n",
    "    def get_top_emotion(self, text):\n",
    "        \"\"\"Get top emotion for text\"\"\"\n",
    "        if text is None or str(text).strip() == \"\":\n",
    "            return {\"emotion_label\": None, \"emotion_score\": np.nan}\n",
    "        \n",
    "        chunks = self.chunk_text(text)\n",
    "        best_label = None\n",
    "        best_score = None\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            result = self.pipeline(chunk)[0]\n",
    "            score = float(result[\"score\"])\n",
    "            if best_score is None or score > best_score:\n",
    "                best_label = result[\"label\"]\n",
    "                best_score = score\n",
    "        \n",
    "        return {\n",
    "            \"emotion_label\": best_label,\n",
    "            \"emotion_score\": round(best_score, 3) if best_score is not None else np.nan\n",
    "        }\n",
    "\n",
    "    def apply_emotions(self):\n",
    "        \"\"\"Get top emotion results and add to df\"\"\"\n",
    "        rows = [self.get_top_emotion(t) for t in self.df[self.text_col]]\n",
    "        emotion_df = pd.DataFrame(rows)\n",
    "        self.df = pd.concat([self.df, emotion_df], axis=1)\n",
    "        return self.df\n",
    "    \n",
    "    def get_specific_emotion_score(self, text, emotion_label):\n",
    "        \"\"\"Get score for specific emotion\"\"\"\n",
    "        if text is None or str(text).strip() == \"\":\n",
    "            return np.nan\n",
    "        \n",
    "        chunks = self.chunk_text(text)\n",
    "        best_score = None\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            out = self.pipeline_top_k(chunk)\n",
    "            if isinstance(out[0], dict):\n",
    "                results = [out[0]]\n",
    "            else:\n",
    "                results = out[0]\n",
    "            \n",
    "            for r in results:\n",
    "                if r[\"label\"].lower() == emotion_label.lower():\n",
    "                    score = float(r[\"score\"])\n",
    "                    if best_score is None or score > best_score:\n",
    "                        best_score = score\n",
    "        \n",
    "        return round(best_score, 3) if best_score is not None else np.nan\n",
    "    \n",
    "    def apply_specific_emotion(self, emotion_label, column_name=None):\n",
    "        \"\"\"Get specific emotion results and add to df\"\"\"\n",
    "        if column_name is None:\n",
    "            column_name = f\"{emotion_label}_score\"\n",
    "        \n",
    "        self.df[column_name] = self.df[self.text_col].apply(\n",
    "            lambda x: self.get_specific_emotion_score(x, emotion_label)\n",
    "        )\n",
    "        return self.df\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0526efbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotAnalyzer:\n",
    "\n",
    "    def __init__(self, df, text_col, date_col):\n",
    "        self.df = df\n",
    "        self.text_col = text_col\n",
    "        self.date_col = date_col\n",
    "\n",
    "    def clean_year(self):\n",
    "        \"\"\"Extract year from date column\"\"\"\n",
    "        def extract_year(raw):\n",
    "            if raw is None:\n",
    "                return np.nan\n",
    "            s = str(raw).lower().strip()\n",
    "            if s == \"\":\n",
    "                return np.nan\n",
    "            s = re.sub(r\"\\[.*?\\]|\\(.*?\\)\", \"\", s)\n",
    "            s = s.replace(\"o.s.\", \"\").replace(\"c.\", \"\").replace(\"ca.\", \"\").replace(\"circa\", \"\")\n",
    "            s = s.replace(\"?\", \"\").strip()\n",
    "            century = re.search(r\"(\\d+)(st|nd|rd|th) century\", s)\n",
    "            if century:\n",
    "                c = int(century.group(1))\n",
    "                return -(c - 1) * 100 if \"bc\" in s else (c - 1) * 100\n",
    "            if \"bc\" in s:\n",
    "                m = re.search(r\"\\d{1,4}\", s)\n",
    "                return -int(m.group()) if m else np.nan\n",
    "            s = s.replace(\"ad\", \"\").strip()\n",
    "            m4 = re.findall(r\"\\b\\d{4}\\b\", s)\n",
    "            if m4:\n",
    "                return int(m4[-1])\n",
    "            m2 = re.search(r\"\\b(\\d{2})\\b$\", s)\n",
    "            if m2:\n",
    "                y = int(m2.group(1))\n",
    "                return 2000 + y if y <= 25 else 1900 + y\n",
    "            return np.nan\n",
    "        \n",
    "        def extract_year_row(row):\n",
    "            for col in [\"date\", \"title\", \"context\"]:\n",
    "                y = extract_year(row[col])\n",
    "                if not pd.isna(y):\n",
    "                    return y\n",
    "            return np.nan\n",
    "        \n",
    "        self.df[\"year\"] = self.df.apply(extract_year_row, axis=1)\n",
    "        self.year_col = self.df[\"year\"]\n",
    "        return self.df\n",
    "    \n",
    "    def clean_binary_columns(self):\n",
    "        \"\"\"Convert binary columns to numeric\"\"\"\n",
    "        self.df[\"is_expected\"] = pd.to_numeric(self.df[\"is_expected\"], errors=\"coerce\")\n",
    "        self.df[\"is_criminal\"] = pd.to_numeric(self.df[\"is_criminal\"], errors=\"coerce\")\n",
    "        self.df[\"is_religious\"] = pd.to_numeric(self.df[\"is_religious\"], errors=\"coerce\")\n",
    "        return self.df\n",
    "    \n",
    "    def filter_by_category(self, category, value):\n",
    "        \"\"\"Filter dataframe by category column value\"\"\"\n",
    "        return self.df[self.df[category] == value]\n",
    "    \n",
    "    def filter_by_year_range(self, start_year=None, end_year=None):\n",
    "        \"\"\"Filter dataframe by year range\"\"\"\n",
    "        if start_year is None and end_year is None:\n",
    "            return self.df\n",
    "        elif start_year is None:\n",
    "            return self.df[self.df[\"year\"] < end_year]\n",
    "        elif end_year is None:\n",
    "            return self.df[self.df[\"year\"] >= start_year]\n",
    "        else:\n",
    "            return self.df[(self.df[\"year\"] >= start_year) & (self.df[\"year\"] < end_year)]\n",
    "    \n",
    "    def create_filtered_dfs(self):\n",
    "        \"\"\"Create all common filtered dataframes and store as attributes\"\"\"\n",
    "        self.criminal_df = self.filter_by_category('is_criminal', 1)\n",
    "        self.not_criminal_df = self.filter_by_category('is_criminal', 0)\n",
    "        self.expected_df = self.filter_by_category('is_expected', 1)\n",
    "        self.not_expected_df = self.filter_by_category('is_expected', 0)\n",
    "        self.religion_df = self.filter_by_category('is_religious', 1)\n",
    "        self.not_religion_df = self.filter_by_category('is_religious', 0)\n",
    "        self.pre_1700_df = self.filter_by_year_range(end_year=1700)\n",
    "        self.df_1700s = self.filter_by_year_range(1700, 1800)\n",
    "        self.df_1800s = self.filter_by_year_range(1800, 1900)\n",
    "        self.df_1900s = self.filter_by_year_range(1900, 2000)\n",
    "        self.df_2000s = self.filter_by_year_range(2000)\n",
    "        return self\n",
    "\n",
    "\n",
    "class WordCloudGenerator(PlotAnalyzer):\n",
    "    def wordcloud(self, sub_df=None, text_col=None, max_words=100, title=\"Word Cloud\"):\n",
    "        from wordcloud import WordCloud\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        if sub_df is None:\n",
    "            sub_df = self.df\n",
    "        \n",
    "        if text_col is None:\n",
    "            text_col = self.text_col\n",
    "        \n",
    "        text = \" \".join(sub_df[text_col].dropna().astype(str).tolist())\n",
    "        wc = WordCloud(\n",
    "            width=800,\n",
    "            height=400,\n",
    "            background_color=\"white\",\n",
    "            max_words=max_words\n",
    "        ).generate(text)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.title(title)\n",
    "        plt.imshow(wc)\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class CompositionChart(PlotAnalyzer):\n",
    "    def plot(self):\n",
    "        \"\"\"Generate stacked bar chart showing dataset composition\"\"\"\n",
    "        groups = {\n",
    "            \"Criminal Status\": [\n",
    "                (\"Criminal\", len(self.criminal_df)),\n",
    "                (\"Not Criminal\", len(self.not_criminal_df)),\n",
    "            ],\n",
    "            \"Expected Death\": [\n",
    "                (\"Expected\", len(self.expected_df)),\n",
    "                (\"Not Expected\", len(self.not_expected_df)),\n",
    "            ],\n",
    "            \"Religious Reference\": [\n",
    "                (\"Religious\", len(self.religion_df)),\n",
    "                (\"Not Religious\", len(self.not_religion_df)),\n",
    "            ],\n",
    "            \"By Century\": [\n",
    "                (\"Pre-1700\", len(self.pre_1700_df)),\n",
    "                (\"1700s\", len(self.df_1700s)),\n",
    "                (\"1800s\", len(self.df_1800s)),\n",
    "                (\"1900s\", len(self.df_1900s)),\n",
    "                (\"2000s\", len(self.df_2000s)),\n",
    "            ],\n",
    "        }\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        x_pos = list(range(len(groups)))\n",
    "        bottom = [0] * len(groups)\n",
    "        \n",
    "        for stack_i in range(max(len(v) for v in groups.values())):\n",
    "            for bar_i, group_items in enumerate(groups.values()):\n",
    "                if stack_i >= len(group_items):\n",
    "                    continue\n",
    "                label, value = group_items[stack_i]\n",
    "                plt.bar(bar_i, value, bottom=bottom[bar_i])\n",
    "                y_center = bottom[bar_i] + value / 2\n",
    "                plt.text(bar_i, y_center, label, ha=\"center\", va=\"center\", fontsize=9)\n",
    "                bottom[bar_i] += value\n",
    "        \n",
    "        plt.xticks(x_pos, list(groups.keys()), rotation=15)\n",
    "        plt.title(\"Dataset Composition by Category\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "class TopEmotionsChart(PlotAnalyzer):\n",
    "    def plot(self, sub_df=None, title=\"Top 10 Emotions (Excluding Neutral)\"):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        if sub_df is None:\n",
    "            sub_df = self.df\n",
    "        \n",
    "        emotion_counts = (\n",
    "            sub_df[\"emotion_label\"]\n",
    "            .loc[sub_df[\"emotion_label\"].str.lower() != \"neutral\"]\n",
    "            .value_counts()\n",
    "            .head(10)\n",
    "            .sort_values(ascending=True)\n",
    "        )\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        emotion_counts.plot(kind=\"barh\")\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Count\")\n",
    "        plt.ylabel(\"Emotion\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class EmotionHist(PlotAnalyzer):\n",
    "    def plot(self, emotion_col, sub_df=None, title=\"Emotion Score Distribution\", bins=10):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        if sub_df is None:\n",
    "            sub_df = self.df\n",
    "        \n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.hist(sub_df[emotion_col].dropna(), bins=bins)\n",
    "        plt.title(title)\n",
    "        plt.xlabel(f\"{emotion_col.replace('_', ' ').title()}\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9189a517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Run emotion analysis\n",
    "emotion_analyzer = EmotionAnalyzer(df, 'quote')\n",
    "df = emotion_analyzer.apply_emotions()\n",
    "df = emotion_analyzer.apply_specific_emotion('remorse')\n",
    "\n",
    "# Step 2: Use the updated df with PlotAnalyzer\n",
    "plot_analyzer = PlotAnalyzer(df, 'quote', 'date')\n",
    "plot_analyzer.clean_year()\n",
    "plot_analyzer.clean_binary_columns()\n",
    "plot_analyzer.create_filtered_dfs()\n",
    "\n",
    "# Step 3: Use all the chart classes\n",
    "# Composition chart\n",
    "comp_chart = CompositionChart(df, 'quote', 'date')\n",
    "comp_chart.create_filtered_dfs()\n",
    "comp_chart.plot()\n",
    "\n",
    "# Word clouds\n",
    "wc_gen = WordCloudGenerator(df, 'quote', 'date')\n",
    "wc_gen.create_filtered_dfs()\n",
    "wc_gen.wordcloud(title=\"All Last Words\")\n",
    "wc_gen.wordcloud(wc_gen.criminal_df, title=\"Criminal Last Words\")\n",
    "\n",
    "# Top emotions chart\n",
    "emotions_chart = TopEmotionsChart(df, 'quote', 'date')\n",
    "emotions_chart.create_filtered_dfs()\n",
    "emotions_chart.plot()\n",
    "emotions_chart.plot(emotions_chart.criminal_df, title=\"Top Emotions - Criminal\")\n",
    "\n",
    "# Emotion histogram\n",
    "hist_chart = EmotionHist(df, 'quote', 'date')\n",
    "hist_chart.create_filtered_dfs()\n",
    "hist_chart.plot(\"remorse_score\", title=\"Remorse Score Distribution - All\")\n",
    "hist_chart.plot(\"remorse_score\", hist_chart.criminal_df, title=\"Remorse - Criminal\")\n",
    "hist_chart.plot(\"emotion_score\", hist_chart.religion_df, title=\"Emotion Score - Religious\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c8dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Emotion Analysis and Visualization Pipeline\")\n",
    "    \n",
    "    parser.add_argument(\"-f\", \"--file\", required=True, help=\"Path to input CSV file\")\n",
    "    parser.add_argument(\"-t\", \"--text_col\", required=True, help=\"Name of text column\")\n",
    "    parser.add_argument(\"-d\", \"--date_col\", required=True, help=\"Name of date column\")\n",
    "    parser.add_argument(\"-o\", \"--output\", help=\"Path to save processed CSV\")\n",
    "    parser.add_argument(\"-e\", \"--emotion\", help=\"Specific emotion to analyze (e.g., remorse, joy)\")\n",
    "    parser.add_argument(\"-m\", \"--model\", default=\"SamLowe/roberta-base-go_emotions\", help=\"Emotion model name\")\n",
    "    parser.add_argument(\"--skip-analysis\", action=\"store_true\", help=\"Skip emotion analysis (use existing columns)\")\n",
    "    parser.add_argument(\"--composition\", action=\"store_true\", help=\"Generate composition chart\")\n",
    "    parser.add_argument(\"--wordcloud\", action=\"store_true\", help=\"Generate word cloud\")\n",
    "    parser.add_argument(\"--top-emotions\", action=\"store_true\", help=\"Generate top emotions chart\")\n",
    "    parser.add_argument(\"--emotion-hist\", action=\"store_true\", help=\"Generate emotion histogram\")\n",
    "    parser.add_argument(\"--all-plots\", action=\"store_true\", help=\"Generate all visualizations\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    # Load data\n",
    "    logging.info(f\"Loading data from {args.file}\")\n",
    "    df = pd.read_csv(args.file)\n",
    "    \n",
    "    # Run emotion analysis\n",
    "    if not args.skip_analysis:\n",
    "        logging.info(\"Beginning emotion analysis\")\n",
    "        emotion_analyzer = EmotionAnalyzer(df, args.text_col, model_name=args.model)\n",
    "        df = emotion_analyzer.apply_emotions()\n",
    "        logging.info(\"Emotion analysis complete\")\n",
    "        \n",
    "        if args.emotion:\n",
    "            logging.info(f\"Adding {args.emotion} scores\")\n",
    "            df = emotion_analyzer.apply_specific_emotion(args.emotion)\n",
    "    \n",
    "    # Set up plot analyzer\n",
    "    logging.info(\"Preparing data for visualization\")\n",
    "    plot_analyzer = PlotAnalyzer(df, args.text_col, args.date_col)\n",
    "    plot_analyzer.clean_year()\n",
    "    plot_analyzer.clean_binary_columns()\n",
    "    plot_analyzer.create_filtered_dfs()\n",
    "    \n",
    "    # Generate visualizations\n",
    "    if args.composition or args.all_plots:\n",
    "        logging.info(\"Generating composition chart\")\n",
    "        comp_chart = CompositionChart(df, args.text_col, args.date_col)\n",
    "        comp_chart.create_filtered_dfs()\n",
    "        comp_chart.plot()\n",
    "    \n",
    "    if args.wordcloud or args.all_plots:\n",
    "        logging.info(\"Generating word cloud\")\n",
    "        wc_gen = WordCloudGenerator(df, args.text_col, args.date_col)\n",
    "        wc_gen.create_filtered_dfs()\n",
    "        wc_gen.wordcloud(title=\"All Last Words\")\n",
    "    \n",
    "    if args.top_emotions or args.all_plots:\n",
    "        logging.info(\"Generating top emotions chart\")\n",
    "        emotions_chart = TopEmotionsChart(df, args.text_col, args.date_col)\n",
    "        emotions_chart.create_filtered_dfs()\n",
    "        emotions_chart.plot()\n",
    "    \n",
    "    if args.emotion_hist or args.all_plots:\n",
    "        emotion_col = f\"{args.emotion}_score\" if args.emotion else \"emotion_score\"\n",
    "        logging.info(f\"Generating histogram for {emotion_col}\")\n",
    "        hist_chart = EmotionHist(df, args.text_col, args.date_col)\n",
    "        hist_chart.create_filtered_dfs()\n",
    "        hist_chart.plot(emotion_col)\n",
    "    \n",
    "    # Save output\n",
    "    if args.output:\n",
    "        logging.info(f\"Saving processed data to {args.output}\")\n",
    "        df.to_csv(args.output, index=False)\n",
    "        logging.info(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsan5400",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
